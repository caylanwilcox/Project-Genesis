# Week 1: Database & Infrastructure Setup - Implementation Plan

**Importance: ğŸ”´ CRITICAL (10/10)**

**Timeline:** 5 days (can be compressed to 3 days with managed database)

---

## Executive Summary

Week 1 establishes the **foundational data infrastructure** for the ML trading prediction system. This week transforms your current frontend-focused Next.js application into a full-stack ML-ready platform with robust data persistence, time-series optimization, and future trading bot integration capabilities.

**Current State:**
- âœ… Next.js 15 frontend with TypeScript
- âœ… Polygon.io integration for real-time market data
- âœ… React-based charting (lightweight-charts, recharts)
- âœ… Zustand for state management
- âŒ No database (all data is ephemeral)
- âŒ No ML prediction storage
- âŒ No historical data persistence
- âŒ No API for external system integration

**Week 1 Goals:**
- âœ… PostgreSQL + TimescaleDB setup (time-series optimization)
- âœ… Complete database schema for ML system
- âœ… Data persistence layer (ORM/query builder)
- âœ… API routes for ML predictions and trading bot integration
- âœ… Performance benchmarks met (100K inserts <5s, 1yr query <500ms)

---

## Current System Architecture Analysis

### Tech Stack
```
Frontend:
â”œâ”€â”€ Next.js 15.5.3 (App Router)
â”œâ”€â”€ React 19.1.1
â”œâ”€â”€ TypeScript 4.9.5
â”œâ”€â”€ Zustand (state management)
â””â”€â”€ Tailwind CSS 4.x

Data Sources:
â”œâ”€â”€ Polygon.io REST API (@polygon.io/client-js)
â””â”€â”€ In-memory caching (30s cache, rate limiting)

UI Components:
â”œâ”€â”€ lightweight-charts (candlestick charts)
â”œâ”€â”€ recharts (statistical charts)
â””â”€â”€ lucide-react (icons)
```

### Current Data Flow
```
User Request â†’ Polygon Service â†’ API Call â†’ Cache â†’ Component State â†’ UI
                     â†“
              Rate Limiting (13s free tier)
              Exponential Backoff (429 handling)
              In-memory cache (30s TTL)
```

### Key Files
- **`src/services/polygonService.ts`** - Market data fetching, caching, rate limiting
- **`src/types/trading.ts`** - Trading interfaces (Order, Trade, Asset, ChartData)
- **`src/types/polygon.ts`** - Polygon.io data types
- **`app/dashboard/page.tsx`** - Main dashboard
- **`app/ticker/[symbol]/page.tsx`** - Individual ticker views

### Current Limitations (Blockers for ML System)
1. **No Data Persistence** - Cannot store historical data for training
2. **No Feature Storage** - Cannot cache computed technical indicators
3. **No Prediction Tracking** - Cannot measure model accuracy over time
4. **No Model Registry** - Cannot version or deploy trained models
5. **No Trading Bot Interface** - Cannot programmatically access predictions
6. **Performance Constraints** - Polygon.io free tier (5 calls/min) too slow for backtesting

---

## Week 1 Architecture Enhancements

### New System Architecture (Post-Week 1)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         USER INTERFACES                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Next.js Frontend  â”‚  Trading Bot API  â”‚  ML Training Scripts       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚                     â”‚                   â”‚
           â–¼                     â–¼                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      API LAYER (Next.js API Routes)                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  /api/market-data   â”‚  /api/predictions   â”‚  /api/trading          â”‚
â”‚  - Historical bars  â”‚  - Get predictions  â”‚  - Execute orders      â”‚
â”‚  - Real-time data   â”‚  - Track accuracy   â”‚  - Get signals         â”‚
â”‚  - Features         â”‚  - Model metadata   â”‚  - Portfolio status    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚                     â”‚                   â”‚
           â–¼                     â–¼                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DATA ACCESS LAYER (Prisma ORM)                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  marketDataRepository  â”‚  predictionRepository  â”‚  tradingRepository â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚                          â”‚                       â”‚
           â–¼                          â–¼                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                PostgreSQL + TimescaleDB (Cloud/Local)                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  market_data (hypertable)     â”‚  predictions (hypertable)           â”‚
â”‚  - OHLCV bars (1m-1M)         â”‚  - ML predictions with confidence   â”‚
â”‚  - 2+ years history           â”‚  - Actual outcomes (accuracy)       â”‚
â”‚  - Continuous compression     â”‚  - Model versioning                 â”‚
â”‚                               â”‚                                     â”‚
â”‚  features (hypertable)        â”‚  models                             â”‚
â”‚  - 100+ technical indicators  â”‚  - Model metadata                   â”‚
â”‚  - RSI, MACD, Bollinger, etc. â”‚  - Training metrics                 â”‚
â”‚  - Pre-computed for speed     â”‚  - Deployment status                â”‚
â”‚                               â”‚                                     â”‚
â”‚  trades (normal table)        â”‚  portfolio (normal table)           â”‚
â”‚  - Executed trades            â”‚  - Current positions                â”‚
â”‚  - P&L tracking               â”‚  - Balance history                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â–²                          â–²
           â”‚                          â”‚
â”Œâ”€â”€ï¿½ï¿½ï¿½â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Data Ingestion     â”‚    â”‚  ML Training        â”‚
â”‚  (Polygon.io ETL)   â”‚    â”‚  (Python/Node.js)   â”‚
â”‚  - Backfill history â”‚    â”‚  - XGBoost          â”‚
â”‚  - Real-time stream â”‚    â”‚  - LSTM             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Enhancements
1. **TimescaleDB Hypertables** - 10x faster time-series queries
2. **API Layer** - RESTful endpoints for trading bots and ML services
3. **Data Access Layer** - Type-safe database operations with Prisma
4. **Separation of Concerns** - Market data, predictions, trading logic isolated
5. **Extensibility** - Easy to add new data sources or ML models

---

## Database Schema Design

### Core Principles
- **Time-series first** - All market data uses TimescaleDB hypertables
- **Type safety** - Prisma schema ensures compile-time type checking
- **Denormalization** - Pre-compute features for fast inference
- **Audit trail** - Track all predictions for accuracy measurement
- **Flexibility** - Schema supports multiple tickers, timeframes, models

### Schema Overview

```prisma
// FILE: prisma/schema.prisma

datasource db {
  provider = "postgresql"
  url      = env("DATABASE_URL")
}

generator client {
  provider = "prisma-client-js"
}

// ============================================================================
// MARKET DATA TABLES (TimescaleDB Hypertables)
// ============================================================================

model MarketData {
  id         String   @id @default(uuid())
  ticker     String   // SPY, QQQ, IWM, UVXY
  timeframe  String   // 1m, 5m, 15m, 1h, 4h, 1d
  timestamp  DateTime // Bar open time (indexed for time-series)

  // OHLCV data
  open       Float
  high       Float
  low        Float
  close      Float
  volume     Float

  // Metadata
  source     String   @default("polygon") // polygon, alpaca, etc.
  createdAt  DateTime @default(now())

  @@unique([ticker, timeframe, timestamp])
  @@index([ticker, timeframe, timestamp(sort: Desc)])
  @@index([timestamp])
  @@map("market_data")
}

// ============================================================================
// FEATURE ENGINEERING (Pre-computed Technical Indicators)
// ============================================================================

model Feature {
  id         String   @id @default(uuid())
  ticker     String
  timeframe  String
  timestamp  DateTime

  // Moving Averages
  sma_20     Float?
  sma_50     Float?
  sma_200    Float?
  ema_12     Float?
  ema_26     Float?

  // Momentum Indicators
  rsi_14     Float?
  macd       Float?
  macd_signal Float?
  macd_hist  Float?
  stoch_k    Float?
  stoch_d    Float?

  // Volatility Indicators
  bb_upper   Float?
  bb_middle  Float?
  bb_lower   Float?
  atr_14     Float?

  // Volume Indicators
  volume_sma_20 Float?
  obv        Float?

  // Price Action
  high_low_ratio Float?
  close_open_ratio Float?

  // Additional Features (100+ total, expandable)
  features   Json?    // Store additional features as JSON for flexibility

  createdAt  DateTime @default(now())

  @@unique([ticker, timeframe, timestamp])
  @@index([ticker, timeframe, timestamp(sort: Desc)])
  @@map("features")
}

// ============================================================================
// ML PREDICTIONS & TRACKING
// ============================================================================

model Prediction {
  id              String   @id @default(uuid())
  ticker          String
  timeframe       String   // Prediction horizon (1m, 5m, 1h, 1d)
  modelId         String   // Reference to trained model
  modelVersion    String   // v1.0.0, v1.1.0, etc.

  // Prediction Details
  predictionTime  DateTime // When prediction was made
  targetTime      DateTime // When prediction is for (predictionTime + horizon)
  direction       String   // UP, DOWN, NEUTRAL
  confidence      Float    // 0.0 to 1.0
  probability     Float    // Predicted probability of direction

  // Price Predictions
  predictedPrice  Float?
  predictedChange Float?   // Predicted % change

  // Actual Outcome (filled later)
  actualDirection String?  // UP, DOWN, NEUTRAL
  actualPrice     Float?
  actualChange    Float?
  correct         Boolean? // Was prediction correct?

  // Metadata
  features        Json?    // Features used for this prediction
  modelOutput     Json?    // Raw model output for debugging

  createdAt       DateTime @default(now())
  updatedAt       DateTime @updatedAt

  model           Model    @relation(fields: [modelId], references: [id])

  @@index([ticker, timeframe, predictionTime(sort: Desc)])
  @@index([modelId, predictionTime])
  @@index([targetTime]) // For looking up predictions to verify
  @@map("predictions")
}

// ============================================================================
// MODEL REGISTRY
// ============================================================================

model Model {
  id              String   @id @default(uuid())
  name            String   // "SPY_1m_XGBoost_v1"
  ticker          String
  timeframe       String
  algorithm       String   // XGBoost, LSTM, Ensemble
  version         String   // Semantic versioning

  // Training Metadata
  trainedAt       DateTime
  trainingDataFrom DateTime
  trainingDataTo   DateTime
  trainingRows    Int

  // Performance Metrics
  accuracy        Float?   // Test set accuracy
  precision       Float?
  recall          Float?
  f1Score         Float?
  sharpeRatio     Float?   // From backtesting
  maxDrawdown     Float?

  // Hyperparameters
  hyperparameters Json

  // Feature Importance
  featureImportance Json?

  // Deployment
  deployedAt      DateTime?
  isActive        Boolean  @default(false)
  modelPath       String?  // S3/local path to serialized model

  // Relations
  predictions     Prediction[]

  createdAt       DateTime @default(now())
  updatedAt       DateTime @updatedAt

  @@unique([ticker, timeframe, algorithm, version])
  @@index([isActive, ticker, timeframe])
  @@map("models")
}

// ============================================================================
// TRADING & PORTFOLIO (Future Trading Bot Integration)
// ============================================================================

model Trade {
  id              String   @id @default(uuid())
  ticker          String
  side            String   // BUY, SELL
  type            String   // MARKET, LIMIT

  // Execution Details
  quantity        Float
  price           Float
  totalValue      Float
  fees            Float    @default(0)

  // Timestamps
  orderTime       DateTime
  executionTime   DateTime?

  // Status
  status          String   // PENDING, FILLED, CANCELED, FAILED

  // Strategy Context
  predictionId    String?  // Link to ML prediction that triggered trade
  strategy        String?  // "ML_Signal", "Manual", "Hedge", etc.

  // P&L (calculated after exit)
  exitPrice       Float?
  exitTime        DateTime?
  profitLoss      Float?
  profitLossPct   Float?

  createdAt       DateTime @default(now())
  updatedAt       DateTime @updatedAt

  @@index([ticker, orderTime(sort: Desc)])
  @@index([status, orderTime])
  @@map("trades")
}

model Portfolio {
  id              String   @id @default(uuid())
  ticker          String   @unique

  // Position
  quantity        Float    @default(0)
  avgEntryPrice   Float?
  currentPrice    Float?
  marketValue     Float?

  // P&L
  unrealizedPnL   Float?
  unrealizedPnLPct Float?
  realizedPnL     Float    @default(0)

  // Risk
  stopLoss        Float?
  takeProfit      Float?

  updatedAt       DateTime @updatedAt

  @@map("portfolio")
}

// ============================================================================
// SYSTEM METADATA
// ============================================================================

model DataIngestionLog {
  id              String   @id @default(uuid())
  ticker          String
  timeframe       String
  source          String   // polygon, alpaca, etc.

  fromDate        DateTime
  toDate          DateTime
  rowsIngested    Int

  status          String   // SUCCESS, PARTIAL, FAILED
  errorMessage    String?

  createdAt       DateTime @default(now())

  @@index([ticker, timeframe, createdAt(sort: Desc)])
  @@map("data_ingestion_logs")
}

model SystemMetric {
  id              String   @id @default(uuid())
  metricName      String
  metricValue     Float
  metadata        Json?
  timestamp       DateTime @default(now())

  @@index([metricName, timestamp(sort: Desc)])
  @@map("system_metrics")
}
```

### TimescaleDB Hypertable Configuration

After schema creation, we'll convert time-series tables to hypertables:

```sql
-- Convert market_data to hypertable (partitioned by timestamp)
SELECT create_hypertable('market_data', 'timestamp',
  chunk_time_interval => INTERVAL '1 week',
  if_not_exists => TRUE
);

-- Convert features to hypertable
SELECT create_hypertable('features', 'timestamp',
  chunk_time_interval => INTERVAL '1 week',
  if_not_exists => TRUE
);

-- Convert predictions to hypertable
SELECT create_hypertable('predictions', 'predictionTime',
  chunk_time_interval => INTERVAL '1 month',
  if_not_exists => TRUE
);

-- Add compression policy (compress data older than 7 days)
SELECT add_compression_policy('market_data', INTERVAL '7 days');
SELECT add_compression_policy('features', INTERVAL '7 days');

-- Add retention policy (optional - keep last 5 years)
SELECT add_retention_policy('market_data', INTERVAL '5 years');

-- Create continuous aggregates for fast queries
CREATE MATERIALIZED VIEW daily_market_summary
WITH (timescaledb.continuous) AS
SELECT
  ticker,
  time_bucket('1 day', timestamp) AS day,
  first(open, timestamp) AS open,
  max(high) AS high,
  min(low) AS low,
  last(close, timestamp) AS close,
  sum(volume) AS volume
FROM market_data
WHERE timeframe = '1m'
GROUP BY ticker, day;

-- Refresh policy for continuous aggregate
SELECT add_continuous_aggregate_policy('daily_market_summary',
  start_offset => INTERVAL '1 month',
  end_offset => INTERVAL '1 hour',
  schedule_interval => INTERVAL '1 hour'
);
```

---

## API Layer Design (Trading Bot Integration)

### API Routes Structure

```
app/api/
â”œâ”€â”€ market-data/
â”‚   â”œâ”€â”€ route.ts                    # GET /api/market-data?ticker=SPY&timeframe=1h&limit=100
â”‚   â”œâ”€â”€ bulk/route.ts               # POST /api/market-data/bulk (batch insert)
â”‚   â””â”€â”€ features/route.ts           # GET /api/market-data/features
â”‚
â”œâ”€â”€ predictions/
â”‚   â”œâ”€â”€ route.ts                    # GET /api/predictions?ticker=SPY&timeframe=1h
â”‚   â”œâ”€â”€ latest/route.ts             # GET /api/predictions/latest (current signals)
â”‚   â”œâ”€â”€ accuracy/route.ts           # GET /api/predictions/accuracy (rolling metrics)
â”‚   â””â”€â”€ webhook/route.ts            # POST /api/predictions/webhook (real-time alerts)
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ route.ts                    # GET /api/models (list active models)
â”‚   â”œâ”€â”€ [id]/route.ts               # GET /api/models/:id (model details)
â”‚   â””â”€â”€ deploy/route.ts             # POST /api/models/deploy (activate model)
â”‚
â”œâ”€â”€ trading/
â”‚   â”œâ”€â”€ signals/route.ts            # GET /api/trading/signals (actionable signals)
â”‚   â”œâ”€â”€ execute/route.ts            # POST /api/trading/execute (place trade)
â”‚   â”œâ”€â”€ portfolio/route.ts          # GET /api/trading/portfolio
â”‚   â””â”€â”€ history/route.ts            # GET /api/trading/history
â”‚
â””â”€â”€ health/
    â””â”€â”€ route.ts                    # GET /api/health (system status)
```

### Example API Route Implementation

```typescript
// FILE: app/api/predictions/latest/route.ts

import { NextRequest, NextResponse } from 'next/server';
import { prisma } from '@/lib/prisma';

export async function GET(request: NextRequest) {
  try {
    const { searchParams } = new URL(request.url);
    const ticker = searchParams.get('ticker');
    const timeframe = searchParams.get('timeframe');

    // Get latest active predictions
    const predictions = await prisma.prediction.findMany({
      where: {
        ticker: ticker || undefined,
        timeframe: timeframe || undefined,
        model: {
          isActive: true,
        },
        targetTime: {
          gte: new Date(), // Only future predictions
        },
      },
      include: {
        model: {
          select: {
            name: true,
            algorithm: true,
            accuracy: true,
            version: true,
          },
        },
      },
      orderBy: {
        predictionTime: 'desc',
      },
      take: 20,
    });

    return NextResponse.json({
      success: true,
      count: predictions.length,
      predictions,
      timestamp: new Date().toISOString(),
    });
  } catch (error) {
    console.error('Error fetching latest predictions:', error);
    return NextResponse.json(
      { success: false, error: 'Failed to fetch predictions' },
      { status: 500 }
    );
  }
}
```

### Trading Bot Integration Example

```typescript
// External trading bot can consume predictions like this:

const response = await fetch('https://your-app.com/api/predictions/latest?ticker=SPY&timeframe=1h', {
  headers: {
    'Authorization': `Bearer ${API_KEY}`,
  },
});

const { predictions } = await response.json();

// Filter high-confidence signals
const strongSignals = predictions.filter(p =>
  p.confidence > 0.75 &&
  p.model.accuracy > 0.65
);

// Execute trades based on signals
for (const signal of strongSignals) {
  if (signal.direction === 'UP') {
    await executeBuyOrder(signal.ticker, signal.confidence);
  }
}
```

---

## Implementation Steps (Day-by-Day Breakdown)

### Day 1: Database Setup & Schema Design
**Duration:** 8 hours (or 2 hours with Supabase)

**Tasks:**
1. âœ… Choose database hosting (Supabase recommended for speed)
2. âœ… Install PostgreSQL + TimescaleDB extension
3. âœ… Set up environment variables (`.env.local`)
4. âœ… Install Prisma: `npm install prisma @prisma/client`
5. âœ… Create Prisma schema (`prisma/schema.prisma`)
6. âœ… Initialize Prisma: `npx prisma init`
7. âœ… Generate Prisma client: `npx prisma generate`
8. âœ… Run migrations: `npx prisma migrate dev --name init`

**Deliverables:**
- âœ… Database running (local or Supabase)
- âœ… Prisma schema created
- âœ… All tables created with proper indexes
- âœ… TimescaleDB hypertables configured

**Decision Point: Database Hosting**

| Option | Pros | Cons | Cost | Setup Time |
|--------|------|------|------|------------|
| **Supabase** (Recommended) | Managed PostgreSQL, built-in TimescaleDB, free tier, auto backups, real-time subscriptions | Vendor lock-in (easy to migrate) | Free (up to 500MB), then $25/mo | 30 min |
| **Railway** | Easy deploy, automatic scaling, good for production | Starts at $5/mo, no free tier | $5-20/mo | 1 hour |
| **Local PostgreSQL** | Full control, free, good for development | Manual setup, no auto backups, not production-ready | Free | 4-6 hours |
| **AWS RDS** | Enterprise-grade, high scalability | Complex setup, expensive | $30+/mo | 4-6 hours |

**Recommendation:** Start with **Supabase** for Week 1-4, migrate to Railway/AWS for production deployment in Week 12.

---

### Day 2: Data Access Layer & ORM Setup
**Duration:** 6 hours

**Tasks:**
1. âœ… Create Prisma client singleton (`lib/prisma.ts`)
2. âœ… Build repository pattern for data access
3. âœ… Create TypeScript interfaces aligned with Prisma schema
4. âœ… Write database utility functions
5. âœ… Set up connection pooling for performance

**File Structure:**
```
lib/
â”œâ”€â”€ prisma.ts              # Prisma client singleton
â”œâ”€â”€ repositories/
â”‚   â”œâ”€â”€ marketData.ts      # CRUD for market data
â”‚   â”œâ”€â”€ features.ts        # Feature storage/retrieval
â”‚   â”œâ”€â”€ predictions.ts     # Prediction tracking
â”‚   â”œâ”€â”€ models.ts          # Model registry
â”‚   â””â”€â”€ trading.ts         # Trades & portfolio
â””â”€â”€ utils/
    â”œâ”€â”€ database.ts        # DB utilities
    â””â”€â”€ validation.ts      # Input validation
```

**Example: Market Data Repository**

```typescript
// FILE: lib/repositories/marketData.ts

import { prisma } from '@/lib/prisma';
import { Prisma } from '@prisma/client';

export class MarketDataRepository {
  /**
   * Insert bulk market data (optimized for speed)
   */
  async bulkInsert(data: Prisma.MarketDataCreateManyInput[]) {
    const startTime = Date.now();

    const result = await prisma.marketData.createMany({
      data,
      skipDuplicates: true, // Avoid errors on duplicate timestamps
    });

    const duration = Date.now() - startTime;
    console.log(`[MarketData] Inserted ${result.count} rows in ${duration}ms`);

    return result;
  }

  /**
   * Get historical bars (optimized query)
   */
  async getHistoricalBars(
    ticker: string,
    timeframe: string,
    limit: number = 100,
    endDate?: Date
  ) {
    return prisma.marketData.findMany({
      where: {
        ticker,
        timeframe,
        timestamp: endDate ? { lte: endDate } : undefined,
      },
      orderBy: {
        timestamp: 'desc',
      },
      take: limit,
    });
  }

  /**
   * Get latest bar for a ticker
   */
  async getLatest(ticker: string, timeframe: string) {
    return prisma.marketData.findFirst({
      where: { ticker, timeframe },
      orderBy: { timestamp: 'desc' },
    });
  }

  /**
   * Check data completeness (for validation)
   */
  async getDataGaps(ticker: string, timeframe: string, fromDate: Date, toDate: Date) {
    // Use raw SQL for complex time-series analysis
    return prisma.$queryRaw`
      SELECT
        ticker,
        timeframe,
        COUNT(*) as total_bars,
        MIN(timestamp) as earliest,
        MAX(timestamp) as latest
      FROM market_data
      WHERE ticker = ${ticker}
        AND timeframe = ${timeframe}
        AND timestamp BETWEEN ${fromDate} AND ${toDate}
      GROUP BY ticker, timeframe
    `;
  }
}

export const marketDataRepo = new MarketDataRepository();
```

**Deliverables:**
- âœ… Prisma client configured
- âœ… Repository classes for all tables
- âœ… Type-safe database operations
- âœ… Connection pooling enabled

---

### Day 3: Historical Data Ingestion (ETL Pipeline)
**Duration:** 8 hours

**Tasks:**
1. âœ… Create data ingestion script
2. âœ… Backfill 2 years of historical data for SPY (test ticker)
3. âœ… Validate data completeness (no gaps)
4. âœ… Benchmark insert performance (target: 100K rows <5s)
5. âœ… Set up automated data refresh (cron job or Next.js API route)

**ETL Script Example:**

```typescript
// FILE: scripts/backfill-market-data.ts

import { polygonService } from '@/src/services/polygonService';
import { marketDataRepo } from '@/lib/repositories/marketData';
import { Prisma } from '@prisma/client';

interface BackfillConfig {
  tickers: string[];
  timeframes: string[];
  yearsBack: number;
  batchSize: number;
}

async function backfillMarketData(config: BackfillConfig) {
  const { tickers, timeframes, yearsBack, batchSize } = config;

  for (const ticker of tickers) {
    for (const timeframe of timeframes) {
      console.log(`\n[Backfill] Starting ${ticker} ${timeframe}...`);

      const endDate = new Date();
      const startDate = new Date();
      startDate.setFullYear(startDate.getFullYear() - yearsBack);

      let currentDate = new Date(startDate);
      let totalInserted = 0;

      while (currentDate < endDate) {
        // Fetch data in chunks (Polygon.io limit: 50K per request)
        const nextDate = new Date(currentDate);
        nextDate.setDate(nextDate.getDate() + 30); // 30-day chunks

        try {
          const bars = await polygonService.getAggregates(
            ticker,
            timeframe as any,
            10000 // Large limit to get all data in date range
          );

          // Transform to Prisma format
          const records: Prisma.MarketDataCreateManyInput[] = bars.map(bar => ({
            ticker,
            timeframe,
            timestamp: new Date(bar.time),
            open: bar.open,
            high: bar.high,
            low: bar.low,
            close: bar.close,
            volume: bar.volume,
            source: 'polygon',
          }));

          // Bulk insert
          if (records.length > 0) {
            const result = await marketDataRepo.bulkInsert(records);
            totalInserted += result.count;
            console.log(`[Backfill] Inserted ${result.count} bars (total: ${totalInserted})`);
          }

          currentDate = nextDate;

          // Rate limiting (respect Polygon.io limits)
          await new Promise(resolve => setTimeout(resolve, 13000));

        } catch (error) {
          console.error(`[Backfill] Error fetching ${ticker} ${timeframe}:`, error);
          // Log error and continue
          await prisma.dataIngestionLog.create({
            data: {
              ticker,
              timeframe,
              source: 'polygon',
              fromDate: currentDate,
              toDate: nextDate,
              rowsIngested: 0,
              status: 'FAILED',
              errorMessage: error.message,
            },
          });
        }
      }

      console.log(`[Backfill] Completed ${ticker} ${timeframe}: ${totalInserted} total bars`);
    }
  }
}

// Run backfill
backfillMarketData({
  tickers: ['SPY'], // Start with SPY for Week 3
  timeframes: ['1m', '5m', '15m', '1h', '4h', '1d'],
  yearsBack: 2,
  batchSize: 10000,
}).then(() => {
  console.log('\nâœ… Backfill complete!');
  process.exit(0);
}).catch(error => {
  console.error('\nâŒ Backfill failed:', error);
  process.exit(1);
});
```

**Performance Optimization:**
```typescript
// Use Prisma's batch insert for maximum speed
await prisma.$transaction(
  records.map(record => prisma.marketData.create({ data: record })),
  { timeout: 60000 }
);

// Alternative: Use raw SQL for even faster inserts
await prisma.$executeRaw`
  INSERT INTO market_data (ticker, timeframe, timestamp, open, high, low, close, volume, source)
  VALUES ${Prisma.join(records.map(r => Prisma.sql`(${r.ticker}, ${r.timeframe}, ${r.timestamp}, ${r.open}, ${r.high}, ${r.low}, ${r.close}, ${r.volume}, ${r.source})`))}
  ON CONFLICT (ticker, timeframe, timestamp) DO NOTHING
`;
```

**Deliverables:**
- âœ… 2+ years of SPY data stored (500K+ bars per timeframe)
- âœ… Data ingestion logs tracked
- âœ… No data gaps validated
- âœ… Insert performance: 100K rows in <5 seconds âœ…

---

### Day 4: API Routes & Trading Bot Integration
**Duration:** 6 hours

**Tasks:**
1. âœ… Create Next.js API routes (market-data, predictions, trading)
2. âœ… Implement authentication (API keys for trading bots)
3. âœ… Add rate limiting (protect endpoints)
4. âœ… Write API documentation (OpenAPI/Swagger)
5. âœ… Test endpoints with Postman/cURL

**API Route Example:**

```typescript
// FILE: app/api/market-data/route.ts

import { NextRequest, NextResponse } from 'next/server';
import { marketDataRepo } from '@/lib/repositories/marketData';
import { z } from 'zod';

// Input validation schema
const querySchema = z.object({
  ticker: z.string().min(1).max(10),
  timeframe: z.enum(['1m', '5m', '15m', '1h', '4h', '1d']),
  limit: z.coerce.number().min(1).max(1000).default(100),
  endDate: z.coerce.date().optional(),
});

export async function GET(request: NextRequest) {
  try {
    // Parse query params
    const { searchParams } = new URL(request.url);
    const params = {
      ticker: searchParams.get('ticker'),
      timeframe: searchParams.get('timeframe'),
      limit: searchParams.get('limit'),
      endDate: searchParams.get('endDate'),
    };

    // Validate input
    const validated = querySchema.parse(params);

    // Fetch data
    const data = await marketDataRepo.getHistoricalBars(
      validated.ticker,
      validated.timeframe,
      validated.limit,
      validated.endDate
    );

    return NextResponse.json({
      success: true,
      ticker: validated.ticker,
      timeframe: validated.timeframe,
      count: data.length,
      data,
    });
  } catch (error) {
    if (error instanceof z.ZodError) {
      return NextResponse.json(
        { success: false, error: 'Invalid parameters', details: error.errors },
        { status: 400 }
      );
    }

    console.error('[API] Error:', error);
    return NextResponse.json(
      { success: false, error: 'Internal server error' },
      { status: 500 }
    );
  }
}
```

**Authentication Middleware:**

```typescript
// FILE: lib/middleware/auth.ts

import { NextRequest, NextResponse } from 'next/server';

const API_KEYS = new Set([
  process.env.TRADING_BOT_API_KEY,
  process.env.ML_SERVICE_API_KEY,
]);

export function withAuth(handler: (req: NextRequest) => Promise<NextResponse>) {
  return async (req: NextRequest) => {
    const apiKey = req.headers.get('Authorization')?.replace('Bearer ', '');

    if (!apiKey || !API_KEYS.has(apiKey)) {
      return NextResponse.json(
        { success: false, error: 'Unauthorized' },
        { status: 401 }
      );
    }

    return handler(req);
  };
}
```

**Deliverables:**
- âœ… API routes deployed
- âœ… Authentication working
- âœ… Rate limiting configured
- âœ… API documentation created

---

### Day 5: Performance Benchmarking & Validation
**Duration:** 6 hours

**Tasks:**
1. âœ… Run performance benchmarks
2. âœ… Validate success criteria
3. âœ… Set up monitoring (optional: Grafana/Prometheus)
4. âœ… Create Week 1 summary report
5. âœ… Prepare for Week 2 (feature engineering setup)

**Benchmark Script:**

```typescript
// FILE: scripts/benchmark-database.ts

import { marketDataRepo } from '@/lib/repositories/marketData';
import { performance } from 'perf_hooks';

async function runBenchmarks() {
  console.log('Starting database performance benchmarks...\n');

  // Test 1: Bulk insert speed (100K rows)
  console.log('Test 1: Bulk Insert (100K rows)');
  const insertData = Array.from({ length: 100000 }, (_, i) => ({
    ticker: 'BENCH',
    timeframe: '1m',
    timestamp: new Date(Date.now() - i * 60000),
    open: 100 + Math.random() * 10,
    high: 105 + Math.random() * 10,
    low: 95 + Math.random() * 10,
    close: 100 + Math.random() * 10,
    volume: 1000000 + Math.random() * 100000,
    source: 'benchmark',
  }));

  const insertStart = performance.now();
  await marketDataRepo.bulkInsert(insertData);
  const insertDuration = performance.now() - insertStart;
  console.log(`âœ… Inserted 100K rows in ${insertDuration.toFixed(0)}ms (${insertDuration < 5000 ? 'PASS' : 'FAIL'})\n`);

  // Test 2: Query speed (1 year of 1m data = ~100K rows)
  console.log('Test 2: Query 1 Year of Data');
  const queryStart = performance.now();
  const oneYearAgo = new Date();
  oneYearAgo.setFullYear(oneYearAgo.getFullYear() - 1);

  const data = await marketDataRepo.getHistoricalBars('SPY', '1m', 100000, new Date());
  const queryDuration = performance.now() - queryStart;
  console.log(`âœ… Queried ${data.length} rows in ${queryDuration.toFixed(0)}ms (${queryDuration < 500 ? 'PASS' : 'FAIL'})\n`);

  // Test 3: Latest bar query (should be <10ms)
  console.log('Test 3: Latest Bar Query');
  const latestStart = performance.now();
  await marketDataRepo.getLatest('SPY', '1m');
  const latestDuration = performance.now() - latestStart;
  console.log(`âœ… Latest bar query in ${latestDuration.toFixed(0)}ms (${latestDuration < 10 ? 'PASS' : 'FAIL'})\n`);

  console.log('Benchmarks complete!');
}

runBenchmarks().then(() => process.exit(0));
```

**Success Criteria Checklist:**
- âœ… PostgreSQL + TimescaleDB running
- âœ… All tables created with proper indexes
- âœ… Can insert 100K rows in <5 seconds
- âœ… Can query 1 year of data in <500ms
- âœ… 2+ years of SPY data stored
- âœ… API routes working
- âœ… Trading bot can fetch predictions via API

---

## Environment Variables Setup

Create `.env.local`:

```env
# Database
DATABASE_URL="postgresql://user:password@localhost:5432/trading_ml?schema=public"
# For Supabase: "postgresql://postgres:[password]@db.[project].supabase.co:5432/postgres"

# Polygon.io
NEXT_PUBLIC_POLYGON_API_KEY="your_polygon_api_key"
NEXT_PUBLIC_POLYGON_PLAN="free"  # or "starter", "developer"

# API Authentication
TRADING_BOT_API_KEY="your_secure_random_key_here"
ML_SERVICE_API_KEY="another_secure_random_key"

# Optional: Monitoring
SENTRY_DSN="your_sentry_dsn"
```

---

## Risk Mitigation & Contingency Plans

### Risk 1: TimescaleDB Not Available on Hosting Provider
**Likelihood:** Low (Supabase/Railway support it)
**Impact:** Medium (slower queries, but system still works)
**Mitigation:**
- Use PostgreSQL without TimescaleDB for Week 1
- Manually create indexes for timestamp-based queries
- Add TimescaleDB later when migrating to production

### Risk 2: Polygon.io Rate Limits Too Restrictive
**Likelihood:** High (free tier = 5 calls/min)
**Impact:** High (data ingestion takes 10+ hours)
**Mitigation:**
- Run backfill over multiple days
- Upgrade to Polygon Starter plan ($29/mo, unlimited historical)
- Use alternative data source (Alpha Vantage, Yahoo Finance)

### Risk 3: Database Insert Performance Below Target
**Likelihood:** Medium
**Impact:** Medium (longer ingestion times)
**Mitigation:**
- Use raw SQL instead of Prisma for bulk inserts
- Disable indexes during bulk insert, re-enable after
- Use PostgreSQL `COPY` command for maximum speed

---

## Week 1 Deliverables Summary

### Technical Deliverables
- âœ… PostgreSQL + TimescaleDB database (cloud or local)
- âœ… Prisma schema with all tables
- âœ… Data access layer (repositories)
- âœ… API routes for trading bot integration
- âœ… 2+ years of SPY historical data
- âœ… Performance benchmarks passed

### Documentation Deliverables
- âœ… Database schema documentation
- âœ… API endpoint documentation
- âœ… Environment setup guide
- âœ… Week 1 summary report

### Validation Criteria
- âœ… Can insert 100K rows in <5 seconds
- âœ… Can query 1 year of data in <500ms
- âœ… No data gaps in historical data
- âœ… API returns predictions in <100ms
- âœ… Trading bot can authenticate and fetch data

---

## Next Steps (Week 2 Preview)

Week 2 focuses on **feature engineering** - transforming raw OHLCV data into 100+ technical indicators:

1. **Compute Features:**
   - Moving averages (SMA, EMA)
   - Momentum indicators (RSI, MACD, Stochastic)
   - Volatility indicators (Bollinger Bands, ATR)
   - Volume indicators (OBV, Volume SMA)
   - Price action features

2. **Store Features:**
   - Populate `features` table
   - Optimize for fast retrieval during inference

3. **Data Quality:**
   - Validate feature calculations
   - Handle missing data
   - Normalize/standardize features

**Preparation for Week 2:**
- Install Python (for TA-Lib technical indicators library)
- Review feature engineering best practices
- Identify most predictive features for SPY

---

## Conclusion

Week 1 transforms your application from a **frontend prototype** to a **production-ready ML platform** with:
- âœ… Persistent data storage (PostgreSQL + TimescaleDB)
- âœ… Scalable time-series architecture
- âœ… API layer for trading bot integration
- âœ… 2+ years of historical data
- âœ… Type-safe database operations
- âœ… Performance-optimized queries

This foundation enables **Week 2 (feature engineering)**, **Week 3 (ML training)**, and **Week 11 (feedback loops)** - the critical path to a working ML trading system.

**Estimated Time:** 5 days (40 hours) or 3 days compressed with Supabase

**Recommended Approach:**
1. Use **Supabase** for Week 1-4 (fast setup, managed database)
2. Backfill SPY data first (test case for Week 3)
3. Build API routes early (enables trading bot development in parallel)
4. Run benchmarks daily to catch performance issues early

**Go/No-Go Decision:**
At end of Week 1, you should have:
- âœ… Database with 2+ years of SPY data
- âœ… API returning data in <100ms
- âœ… No data gaps or quality issues

If any criteria fails, **STOP** and debug before Week 2. Week 2-3 depend entirely on Week 1's data quality.

---

**Ready to begin Week 1? Let's start with database selection and setup!**
